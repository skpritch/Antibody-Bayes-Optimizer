{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import dependencies\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from pathlib import Path\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Configuration\n",
    "\n",
    "INPUT_JSON  = \"../data/processed/input_sequences.json\"\n",
    "OUTPUT_JSON = \"../processed/selected_ngs/sele_ngs.json\"\n",
    "N_SELECT    = 1200            # total sequences to select\n",
    "MIN_CLUSTER_SIZE = 1000         # HDBSCAN parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load & Parse Data\n",
    "\n",
    "df = pd.read_json(INPUT_JSON) \n",
    "assert {\"sequence\", \"embedding_pca\", \"enrichment\"}.issubset(df.columns), \\\n",
    "       \"Input JSON must contain 'sequence', 'embedding_pca', 'enrichment'.\"\n",
    "\n",
    "# Convert stringified embeddings to lists if needed\n",
    "def parse_embedding(val):\n",
    "    if isinstance(val, str):\n",
    "        return ast.literal_eval(val)\n",
    "    return val\n",
    "\n",
    "df[\"embedding\"] = df[\"embedding_pca\"].apply(parse_embedding)\n",
    "\n",
    "# Stack embeddings into array\n",
    "emb_matrix = np.vstack(df[\"embedding\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Expand Data by âˆšEnrichment\n",
    "\n",
    "replicas = np.floor(np.sqrt(df[\"enrichment\"])).astype(int)\n",
    "replicas = replicas.clip(lower=1)\n",
    "\n",
    "# Repeat rows according to replicas\n",
    "df_expanded = df.loc[df.index.repeat(replicas)].reset_index(drop=True)\n",
    "# Repeat embeddings similarly\n",
    "emb_expanded = np.repeat(emb_matrix, replicas, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Cluster with HDBSCAN\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=MIN_CLUSTER_SIZE)\n",
    "labels = clusterer.fit_predict(emb_expanded)\n",
    "df_expanded[\"cluster\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Select Top Sequences per Cluster\n",
    "\n",
    "# Identify non-noise clusters\n",
    "clusters = [c for c in np.unique(labels) if c >= 0]\n",
    "if not clusters:\n",
    "    # fallback: treat all as one cluster\n",
    "    df_expanded[\"cluster\"] = 0\n",
    "    clusters = [0]\n",
    "\n",
    "per_cluster = int(np.ceil(N_SELECT / len(clusters)))\n",
    "\n",
    "selected = []\n",
    "print(len(clusters))\n",
    "for c in clusters:\n",
    "    sub = df_expanded[df_expanded[\"cluster\"] == c]\n",
    "    # sort by enrichment, drop duplicate sequences\n",
    "    top = (\n",
    "        sub.sort_values(\"enrichment\", ascending=False)\n",
    "           .drop_duplicates(\"sequence\")\n",
    "           .head(per_cluster)\n",
    "    )\n",
    "    selected.extend(top[\"sequence\"].tolist())\n",
    "\n",
    "# Deduplicate and limit to N_SELECT\n",
    "selected = list(dict.fromkeys(selected))[:N_SELECT]\n",
    "\n",
    "# If underfilled, sample from noise cluster (-1)\n",
    "if len(selected) < N_SELECT:\n",
    "    noise = df_expanded[df_expanded[\"cluster\"] == -1]\n",
    "    noise_top = (\n",
    "        noise.sort_values(\"enrichment\", ascending=False)\n",
    "             .drop_duplicates(\"sequence\")\n",
    "    )\n",
    "    for seq in noise_top[\"sequence\"]:\n",
    "        if seq not in selected:\n",
    "            selected.append(seq)\n",
    "        if len(selected) == N_SELECT:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Save Selected Sequences\n",
    "\n",
    "# Filter original df for selected sequences\n",
    "final_df = df[df[\"sequence\"].isin(selected)].reset_index(drop=True)\n",
    "\n",
    "# Map each sequence to its cluster label (from df_expanded)\n",
    "# We take the first cluster seen for each sequence\n",
    "cluster_map = (\n",
    "    df_expanded\n",
    "      .drop_duplicates(\"sequence\")\n",
    "      .set_index(\"sequence\")[\"cluster\"]\n",
    ")\n",
    "final_df[\"cluster\"] = final_df[\"sequence\"].map(cluster_map)\n",
    "\n",
    "# Flag which sequences made it into r1_test\n",
    "final_df[\"r1_test\"] = True\n",
    "\n",
    "# Write to JSON (now with 'cluster' and 'r1_test' columns)\n",
    "final_df.to_json(OUTPUT_JSON, orient=\"records\", lines=True)\n",
    "\n",
    "print(f\"Selected {len(final_df)} sequences saved to {OUTPUT_JSON}\")\n",
    "final_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes_opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
