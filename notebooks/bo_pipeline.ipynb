{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3bf145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/antibody_bo/pipeline/bo_notebook_runner.py\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from os import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "from utils.utils import load_or_compute_embeddings, load_or_compute_developability, toy_ground_truth\n",
    "from models.mf_gp_model import MultiFidelityGP\n",
    "from models.gp_model import DevelopabilityGP\n",
    "from acquisition.acq_utils import make_acq\n",
    "from acquisition.mutate_seq import hill_climb, genetic_algorithm, gibbs_sampling\n",
    "from embeddings.esm_encoder import embed_sequences, embed_single\n",
    "from developability.dev_score import score_sequences\n",
    "\n",
    "\n",
    "FID_MAP = {\"y_low\": 0.3, \"y_medium\": 0.7, \"y_high\": 0.95}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6e747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bo(\n",
    "  cfg,\n",
    "  raw_json: str | Path,\n",
    "  embed_file: str | Path,\n",
    "  dev_file: str | Path,\n",
    "  dev_dir: str | Path,\n",
    "  ground_truth_fn,\n",
    "):\n",
    "  # STEP 1: embeddings with cache\n",
    "  print(\"→ Embedding sequences…\")\n",
    "  df = load_or_compute_embeddings(\n",
    "      raw_json,\n",
    "      embed_file,\n",
    "      seq_col=\"sequence\",\n",
    "      embed_col=\"pca_embed\",\n",
    "      embed_fn=lambda d: embed_sequences(d, n_components=cfg.embed_components),\n",
    "  )\n",
    "\n",
    "  df[\"seq_id\"] = list(range(len(df)))\n",
    "  next_seq_id = len(df)\n",
    "  \n",
    "\n",
    "  \"\"\"# STEP 2: developability with cache\n",
    "  print(\"→ Computing developability…\")\n",
    "  df = load_or_compute_developability(\n",
    "      embed_file,\n",
    "      dev_file,\n",
    "      seq_col=\"sequence\",\n",
    "      dev_col=\"dev_score\",\n",
    "      score_fn=score_sequences,\n",
    "      dev_json_dir=dev_dir\n",
    "  )\"\"\"\n",
    "\n",
    "  # Scale Dev Scores on 0-1\n",
    "  dev_vals = df[\"dev_score\"].values\n",
    "  mask_dev = ~np.isnan(dev_vals)\n",
    "  if mask_dev.any():\n",
    "      d = dev_vals[mask_dev]\n",
    "      d_min, d_max = d.min(), d.max()\n",
    "      df.loc[mask_dev, \"dev_score\"] = ((d - d_min) / (d_max - d_min)).tolist()\n",
    "\n",
    "  # STEP 3: prepare original training data\n",
    "  print(\"→ Initializing GP models…\")\n",
    "  X0 = np.vstack(df[\"pca_embed\"].values).astype(float)\n",
    "  Y0 = df[[\"y_low\", \"y_medium\", \"y_high\"]].values.astype(float)\n",
    "  n = len(df)\n",
    "  # fidelity codes: 0=low,1=medium,2=high\n",
    "  F0 = np.concatenate([np.zeros(n), np.ones(n), np.full(n, 2)])\n",
    "\n",
    "  # initial models\n",
    "  X_mf = np.repeat(X0, 3, axis=0)\n",
    "  y_mf = Y0.ravel()\n",
    "  mf_gp = MultiFidelityGP(X_mf, y_mf, F0, [0, 1, 2])\n",
    "  print(\"  ✔ Fitness MF_GP Model ready.\")\n",
    "\n",
    "\n",
    "  mask_dev = ~np.isnan(df.dev_score.values)\n",
    "  dev_gp = DevelopabilityGP(X0[mask_dev], df.dev_score.values[mask_dev])\n",
    "\n",
    "  print(\"  ✔ Developability GP model ready.\")\n",
    "\n",
    "  # STEP 4: caching helpers\n",
    "  pad_len = max(len(s) for s in df.sequence)\n",
    "  from functools import lru_cache\n",
    "\n",
    "  @lru_cache(maxsize=200_000)\n",
    "  def embed_cached(seq: str) -> np.ndarray:\n",
    "      return embed_single(seq, cfg.embed_components)\n",
    "\n",
    "  @lru_cache(maxsize=200_000)\n",
    "  def fit_one(seq: str) -> float:\n",
    "      return float(mf_gp.predict(embed_cached(seq).reshape(1, -1))[0])\n",
    "\n",
    "  # prepare output container and initial seed\n",
    "  df_all = df.assign(\n",
    "      iter=0, batch=0, selected=True,\n",
    "      y_high=np.nan, dev_score=np.nan\n",
    "  )\n",
    "  seed_seq = df.sequence.iloc[int(np.argmax(Y0[:, 2]))]\n",
    "\n",
    "  processed_path = Path(\"../data/processed/cd98_10k/cd98_vs_interim_1024_low.jsonl\")\n",
    "\n",
    "  # STEP 5: BO loop\n",
    "  for it in tqdm(range(cfg.n_iter), desc=\"BO Iterations\"):\n",
    "\n",
    "      new_rows: list[pd.DataFrame] = []\n",
    "     \n",
    "      for b in tqdm(range(cfg.batch_k), desc=f\" Batches (iter {it+1})\", leave=False):\n",
    "\n",
    "          acq_fn = make_acq(cfg.acq, mf_gp, mf_gp.y.max(), xi=cfg.xi, kappa=cfg.kappa)\n",
    "\n",
    "          # 1) generate & log candidates\n",
    "          if cfg.seq_opt == \"hc\":\n",
    "              recs = hill_climb(\n",
    "                  seed_seq,\n",
    "                  fit_one,\n",
    "                  local_k2_samples=cfg.seq_proposals or len(df),\n",
    "                  restarts=1\n",
    "              )\n",
    "          elif cfg.seq_opt == \"ga\":\n",
    "              recs = genetic_algorithm(\n",
    "                  [seed_seq],\n",
    "                  fit_one,\n",
    "                  max_gen=cfg.ga_generations or 100,\n",
    "                  pop_size=cfg.seq_proposals or len(df),\n",
    "              )\n",
    "          else:\n",
    "              recs = gibbs_sampling(\n",
    "                  seed_seq,\n",
    "                  fit_one,\n",
    "                  gamma=cfg.gamma,\n",
    "                  iters=cfg.gibbs_iters or 10,\n",
    "              )\n",
    "\n",
    "          batch_df = pd.DataFrame(recs)\n",
    "          batch_df[\"seq_id\"] = list(range(next_seq_id, next_seq_id+len(batch_df)))\n",
    "          next_seq_id += len(batch_df)\n",
    "\n",
    "          # attach embeddings for each candidate so pca_embed is always defined\n",
    "          batch_df[\"pca_embed\"] = batch_df[\"sequence\"].apply(\n",
    "              lambda s: embed_cached(s).tolist()\n",
    "          )\n",
    "\n",
    "          # 2) pick one via acquisition\n",
    "          already_selected = set(df_all[df_all.selected].sequence)\n",
    "          unique_seqs = [s for s in batch_df.sequence.unique() if s not in already_selected]\n",
    "          embs        = np.vstack([embed_cached(s) for s in unique_seqs])\n",
    "          f_acq       = acq_fn(embs)\n",
    "\n",
    "          # normalize acquisition scores to [0,1]\n",
    "          f_min, f_max = f_acq.min(), f_acq.max()\n",
    "          if f_max > f_min:\n",
    "              f_norm = (f_acq - f_min) / (f_max - f_min)\n",
    "          else:\n",
    "              # all values identical → give them zero (or 0.5 if you prefer neutral)\n",
    "              f_norm = np.zeros_like(f_acq)\n",
    "\n",
    "          dev_pred    = dev_gp.predict(embs)\n",
    "          combo       = (1 - cfg.dev_weight) * f_acq + cfg.dev_weight * dev_pred\n",
    "          idx         = int(np.argmax(combo))\n",
    "          seq_sel     = unique_seqs[idx]\n",
    "          x_sel       = embs[idx].reshape(1, -1)\n",
    "\n",
    "          # 3) measure true values\n",
    "          y_sel      = ground_truth_fn(x_sel)[0]\n",
    "          seq_id_sel = batch_df[\"seq_id\"].iloc[idx]\n",
    "          dev_true   = score_sequences([seq_sel], [seq_id_sel], dev_dir = Path(dev_dir))[0]\n",
    "\n",
    "\n",
    "          # 4) annotate\n",
    "          batch_df[\"y_high\"]    = np.nan\n",
    "          batch_df[\"dev_score\"] = np.nan\n",
    "          mask = batch_df.sequence == seq_sel\n",
    "\n",
    "          batch_df.loc[mask, \"y_high\"]    = y_sel\n",
    "          batch_df.loc[mask, \"dev_score\"] = dev_true\n",
    "          batch_df[\"iter\"]     = it + 1\n",
    "          batch_df[\"batch\"]    = b  + 1\n",
    "          batch_df[\"selected\"] = mask\n",
    "\n",
    "          sel_row = batch_df[batch_df.selected].copy()\n",
    "          new_rows.append(sel_row)\n",
    "\n",
    "      # 5) append & save interim JSON\n",
    "      df_all = pd.concat([df_all, *new_rows], ignore_index=True)\n",
    "      df_all[df_all.selected].to_json(processed_path, orient=\"records\", lines=True)\n",
    "      new_rows.clear()\n",
    "\n",
    "      # 6) rebuild GPs via original constructors on updated data\n",
    "      sel_df = df_all[df_all.selected]\n",
    "      # multi-fidelity: original + all selected\n",
    "      X_sel = np.vstack(sel_df[\"pca_embed\"].tolist()).astype(float)\n",
    "      y_sel_all = sel_df[sel_df.selected][\"y_high\"].values.astype(float)\n",
    "\n",
    "      X_mf = np.vstack([np.repeat(X0, 3, axis=0), X_sel])\n",
    "      y_mf = np.concatenate([Y0.ravel(), y_sel_all])\n",
    "      F_mf = np.concatenate([F0, np.full(len(y_sel_all), 2)])\n",
    "      mf_gp = MultiFidelityGP(X_mf, y_mf, F_mf, [0, 1, 2])\n",
    "      # developability: original + all selected\n",
    "      \"\"\"y_dev_all = df_upd[df_upd.selected][\"dev_score\"].values.astype(float)\n",
    "      dev_gp = DevelopabilityGP(\n",
    "          np.vstack([X0, X_sel]),\n",
    "          np.concatenate([df.dev_score.values, y_dev_all])\n",
    "      )\"\"\"\n",
    "      mask_orig = ~np.isnan(df.dev_score.values)\n",
    "      X_orig = X0[mask_orig]\n",
    "      y_orig = df.dev_score.values[mask_orig]\n",
    "      X_train = np.vstack([X_orig, X_sel])\n",
    "      y_train = np.concatenate([y_orig, y_sel_all])\n",
    "      dev_gp = DevelopabilityGP(X_train, y_train)\n",
    "         \n",
    "      # 7) next seed\n",
    "      seed_seq = seq_sel\n",
    "\n",
    "  print(\"→ Bayesian optimization complete.\")\n",
    "  return df_all, mf_gp.y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e43882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Embedding sequences…\n",
      "Embeddings ready: 64 / 64\n",
      "→ Computing developability…\n",
      "Developability ready: 64 / 64\n",
      "→ Initializing GP models…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonpritchard/miniconda3/envs/bayes_opt/lib/python3.10/site-packages/botorch/models/utils/assorted.py:270: InputDataWarning: Data (input features) is not contained to the unit cube. Please consider min-max scaling the input data.\n",
      "  check_min_max_scaling(\n",
      "/Users/simonpritchard/miniconda3/envs/bayes_opt/lib/python3.10/site-packages/botorch/models/utils/assorted.py:273: InputDataWarning: Data (outcome observations) is not standardized (std = tensor([0.], dtype=torch.float64), mean = tensor([0.], dtype=torch.float64)).Please consider scaling the input to zero mean and unit variance.\n",
      "  check_standardization(Y=train_Y, raise_on_fail=raise_on_fail)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✔ GP models ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BO Iterations:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Gibbs iterations: 100%|██████████| 3/3 [00:27<00:00,  9.19s/it]\n",
      "/Users/simonpritchard/miniconda3/envs/bayes_opt/lib/python3.10/site-packages/botorch/models/utils/assorted.py:270: InputDataWarning: Data (input features) is not contained to the unit cube. Please consider min-max scaling the input data.\n",
      "  check_min_max_scaling(\n",
      "/Users/simonpritchard/miniconda3/envs/bayes_opt/lib/python3.10/site-packages/botorch/models/utils/assorted.py:273: InputDataWarning: Data (outcome observations) is not standardized (std = tensor([0.], dtype=torch.float64), mean = tensor([0.], dtype=torch.float64)).Please consider scaling the input to zero mean and unit variance.\n",
      "  check_standardization(Y=train_Y, raise_on_fail=raise_on_fail)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Gibbs iterations: 100%|██████████| 3/3 [00:40<00:00, 13.38s/it]\n",
      "/Users/simonpritchard/miniconda3/envs/bayes_opt/lib/python3.10/site-packages/botorch/models/utils/assorted.py:270: InputDataWarning: Data (input features) is not contained to the unit cube. Please consider min-max scaling the input data.\n",
      "  check_min_max_scaling(\n",
      "/Users/simonpritchard/miniconda3/envs/bayes_opt/lib/python3.10/site-packages/botorch/models/utils/assorted.py:273: InputDataWarning: Data (outcome observations) is not standardized (std = tensor([0.], dtype=torch.float64), mean = tensor([0.], dtype=torch.float64)).Please consider scaling the input to zero mean and unit variance.\n",
      "  check_standardization(Y=train_Y, raise_on_fail=raise_on_fail)\n",
      "BO Iterations:  50%|█████     | 1/2 [01:09<01:09, 69.70s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Gibbs iterations: 100%|██████████| 3/3 [00:33<00:00, 11.31s/it]\n",
      "/Users/simonpritchard/miniconda3/envs/bayes_opt/lib/python3.10/site-packages/botorch/models/utils/assorted.py:270: InputDataWarning: Data (input features) is not contained to the unit cube. Please consider min-max scaling the input data.\n",
      "  check_min_max_scaling(\n",
      "/Users/simonpritchard/miniconda3/envs/bayes_opt/lib/python3.10/site-packages/botorch/models/utils/assorted.py:273: InputDataWarning: Data (outcome observations) is not standardized (std = tensor([0.], dtype=torch.float64), mean = tensor([0.], dtype=torch.float64)).Please consider scaling the input to zero mean and unit variance.\n",
      "  check_standardization(Y=train_Y, raise_on_fail=raise_on_fail)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Gibbs iterations: 100%|██████████| 3/3 [00:37<00:00, 12.50s/it]\n",
      "/Users/simonpritchard/miniconda3/envs/bayes_opt/lib/python3.10/site-packages/botorch/models/utils/assorted.py:270: InputDataWarning: Data (input features) is not contained to the unit cube. Please consider min-max scaling the input data.\n",
      "  check_min_max_scaling(\n",
      "/Users/simonpritchard/miniconda3/envs/bayes_opt/lib/python3.10/site-packages/botorch/models/utils/assorted.py:273: InputDataWarning: Data (outcome observations) is not standardized (std = tensor([0.], dtype=torch.float64), mean = tensor([0.], dtype=torch.float64)).Please consider scaling the input to zero mean and unit variance.\n",
      "  check_standardization(Y=train_Y, raise_on_fail=raise_on_fail)\n",
      "BO Iterations: 100%|██████████| 2/2 [02:23<00:00, 71.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Bayesian optimization complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'@dataclass\\nclass PipelineConfig:\\n    acq: str = \"ei\"\\n    seq_opt: str = \"gs\"\\n    seq_proposals: int | None = None\\n    dev_weight: float = 0.3\\n    xi: float = 0.01\\n    kappa: float = 2.0\\n    n_iter: int = 1\\n    batch_k: int = 1\\n    embed_components: int = 64\\n    bounds_scale: float = 1.0\\n\\ncfg = PipelineConfig()\\n\\ndf_all, fitness_history = run_bo(cfg, \"../data/raw/cd98_64_seq.json\", \"../data/interim/cd98_64_embed.jsonl\", \"../data/interim/cd98_64_biophi.jsonl\", toy_ground_truth)\\ndf_all.to_json(\"../data/processed/cd98_64_2.jsonl\", orient=\"records\", lines=True,)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class PipelineConfig:\n",
    "  acq: str = \"ei\"\n",
    "  seq_opt: str = \"gs\"\n",
    "  seq_proposals: int | None = None\n",
    "  dev_weight: float = 0.3\n",
    "  xi: float = 0.1\n",
    "  kappa: float = 0.0\n",
    "  n_iter: int = 4\n",
    "  batch_k: int = 50\n",
    "  embed_components: int = 1024\n",
    "  bounds_scale: float = 1.0\n",
    "  gamma: float = 10.0\n",
    "  gibbs_iters: int = 5\n",
    "  ga_generations: int | None = None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  cfg = PipelineConfig()\n",
    "  df_all, fitness_history = run_bo(\n",
    "      cfg,\n",
    "      \"../data/interim/cd98_10k/cd98_biophi_1024.jsonl\",\n",
    "      \"../data/interim/cd98_10k/cd98_biophi_1024.jsonl\",\n",
    "      \"../data/interim/cd98_10k/cd98_biophi_new.jsonl\",\n",
    "      \"../data/biophi/cd98_10k\",\n",
    "      toy_ground_truth,\n",
    "  )\n",
    "  df_all.to_json(\n",
    "      \"../data/processed/cd98_10k/cd98_final_1024_low.jsonl\",\n",
    "      orient=\"records\",\n",
    "      lines=True,\n",
    "  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes_opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
